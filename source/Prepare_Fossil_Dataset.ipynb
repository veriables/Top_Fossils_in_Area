{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "substantial-turner",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from geopy.geocoders import Nominatim \n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-visibility",
   "metadata": {},
   "source": [
    "### Download the dataset from paleobiodb.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-stopping",
   "metadata": {},
   "source": [
    "1. Create an account on paleobiodb.org\n",
    "2. Go to the downloads page\n",
    "3. Set some parameters to generate your download URL\n",
    "4. What do you want to download? Specimens\n",
    "5. Format: Comma-separated values\n",
    "6. Select by metadata -> check \"Select all specimen records in the database\"\n",
    "7. Choose output options -> check only \"classification\" and \"coordinates\"\n",
    "8. Scroll up to the \"Test\" and \"Download\" buttons and you will see the generated URL\n",
    "9. Click the download button\n",
    "10. Copy it to your project's data folder and rename it pbdb_data_world.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-artist",
   "metadata": {},
   "source": [
    "### Shrink the dataset\n",
    "The original download is too large (~2.15GB).  It contains many fields that we don't need and many rows that we cannot use because they don't have values for latitude or longitude, etc.\n",
    "\n",
    "Also, this dataset is too large to store in GitHub (which has a 100 MB filesize limit).\n",
    "\n",
    "So, in this step, we'll shrink it down to the dataset that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the original large dataset\n",
    "df = pd.read_csv('../data/pbdb_data_world.csv')\n",
    "\n",
    "# Limit the data to just what we need\n",
    "df = df[['genus','accepted_name','lat','lng','state','county']]\n",
    "\n",
    "# Drop any rows that contain NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Save the new file\n",
    "df.to_csv('../data/name_and_location.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-bronze",
   "metadata": {},
   "source": [
    "### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/name_and_location.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-welcome",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many records are there?\n",
    "print('df.shape: {}'.format(df.shape))\n",
    "\n",
    "## How many unique latitude and longitude pairs are there in the records?\n",
    "column_values = df[['lat','lng']].values.ravel()\n",
    "unique_lat_lng_values =  pd.unique(column_values)\n",
    "print('len(unique_lat_lng_values): {}'.format(len(unique_lat_lng_values)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-status",
   "metadata": {},
   "source": [
    "### Decide to implement a two-step process\n",
    "\n",
    "The dataframe is missing a column for country.  I'd like that \"country\" column so I can easily filter the dataframe by country, state/province, and county.  I can calculate it from the latitude and longitude using the geolocation features of the geopy library.  However, labeling each of these records with a \"country\" will be a fair amount of work for the program.\n",
    "\n",
    "Given that the number of unique latitude/longitude pairs is around 1/10th of the number of total records, it would be much better to create a dictionary with a lat/lng key and a country value for those unique lat/lng pairs.  Then, as a second step, use that dictionary to populate a new \"country\" column in the dataframe.  The dictionary lookups will be much faster because the dictionary object uses a HashTable algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-charger",
   "metadata": {},
   "source": [
    "### Step 1: Build the country lookup dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will call the geolocator library to return the country\n",
    "def getCountry(lat, lng):\n",
    "    country = ''\n",
    "    try:\n",
    "        location = geolocator.reverse(str(round(lat, 2))+\",\"+str(round(lng))) \n",
    "        address = location.raw['address'] \n",
    "        country = address.get('country', '') \n",
    "    except:\n",
    "        country = ''\n",
    "    finally:\n",
    "        return country\n",
    "\n",
    "# Use the function to label each unique lat/lng combination with its country\n",
    "country_lookup = {}\n",
    "counter = 0\n",
    "while counter < unique_values.size:\n",
    "    lat = unique_values[counter]\n",
    "    lng = unique_values[counter + 1]\n",
    "    country_lookup[str(lat) + ',' + str(lng)] = getCountry(lat, lng)\n",
    "    if counter % 5000 == 0:\n",
    "        print('counter = {}'.format(counter))\n",
    "    counter += 2\n",
    "\n",
    "# This took about 12 hours so, to make it easier to load in the future without\n",
    "# redoing all this work, I'm going to save this dictionary.\n",
    "pickle.dump( country_lookup, open( \"../data/country_lookup.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-statistics",
   "metadata": {},
   "source": [
    "### Use the country_lookup dictionary to populate a new \"country\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookupCountry(row):\n",
    "    lat, lng = row['lat'], row['lng']\n",
    "    try: \n",
    "        country = country_lookup[str(lat) + ',' + str(lng)] \n",
    "        return country\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "df['country'] = df.apply(lookupCountry, axis=1)\n",
    "\n",
    "# Save the dataframe (with the new country column) for easy loading later\n",
    "df.to_csv('../data/name_and_location_with_country.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Geo)",
   "language": "python",
   "name": "geo_001"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
